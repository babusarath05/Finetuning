{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Install Libraries----------------#\n",
    "# pip install torch\n",
    "# pip install transformers\n",
    "# pip install bitsandbytes\n",
    "# pip install accelerate\n",
    "# pip install datasets\n",
    "# pip install peft\n",
    "# pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Set Environment-----------------#\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/Downloads/SB/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#-----------Read Token-----------------------#\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Model Testing------------------#\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning in less than 100 words.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning with QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Fine Tuning with QLORA---------------------#\n",
    "\n",
    "from datasets import load_dataset,Dataset\n",
    "\n",
    "# # # Load Dolly Dataset\n",
    "# dataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "df = dataset.to_pandas()\n",
    "# df['messages']=[[{\"content\":i,\"role\":\"user\"},{\"content\":j,\"role\":\"assistant\"}] for i,j in df[['prompt','completion']].values]\n",
    "df['messages']=[[{\"content\":f\"context:{i}\\n\\nquestion:{j}\",\"role\":\"user\"},{\"content\":k,\"role\":\"assistant\"}] for i,j,k in df[['context','question','answer']].values]\n",
    "# df = df.drop(columns=['prompt','completion','input_ids','attention_mask','labels'],axis=1)\n",
    "df = df.drop(columns=['context','question','answer'],axis=1)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset[3]['messages'])\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "tokenizer_id = \"philschmid/gemma-tokenizer-chatml\"\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=6,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"SQLgemma-2-2b-it\", # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    #report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1512 # max sequence length for model and packing of the dataset\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Inference------------------#\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,pipeline\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "peft_model_id =  \"/Downloads/SB/SQLgemma-2-2b-it\"\n",
    "# Load adapted model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Get token id for end of conversation\n",
    "# eos_token = tokenizer(\"\",add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "prompts = [\n",
    "    # \"What is Fine Tuning? Explain why thats the case and if it was different in the past?\",\n",
    "    # \"Write a Python function to calculate the factorial of a number.\",\n",
    "    # \"What is the sql query to find the employee with the lowest salary in a table named employee\",\n",
    "    \"context:CREATE TABLE city (City_ID VARCHAR, Population INTEGER); CREATE TABLE farm_competition (Theme VARCHAR, Host_city_ID VARCHAR)\\n\\nquestion:Please show the themes of competitions with host cities having populations larger than 5000.\"\n",
    "]\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.01, top_k=50, top_p=0.95)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "# Test inference\n",
    "for prompt in prompts:\n",
    "    print(f\"    Prompt:\\n{prompt}\")\n",
    "    print(f\"    Response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Base model with Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Merging the Base model with Adapter-----#\n",
    "base_model_url = \"google/gemma-2-2b-it\"\n",
    "new_model_url = \"/Downloads/SB/SQLgemma-2-2b-it\"\n",
    "tokenizer_url = \"/Downloads/SB/SQLgemma-2-2b-it\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from trl import setup_chat_format\n",
    "\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_url)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_url,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, new_model_url)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"SQLGemma-2-2b-it\")\n",
    "tokenizer.save_pretrained(\"SQLGemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Inference and pushing the model to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Final Inference-----------------#\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SQLGemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"SQLGemma-2-2b-it\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompts = [\n",
    "    # \"What is Fine Tuning? Explain why thats the case and if it was different in the past?\",\n",
    "    # \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"context: CREATE TABLE employee (emp_id VARCHAR,emp_name VARCHAR,emp_salary INTEGER,emp_department VARCHAR);\\n\\nquestion:What is the sql query to find the employee name with the lowest salary?\",\n",
    "    # \"context:CREATE TABLE city (City_ID VARCHAR, Population INTEGER); CREATE TABLE farm_competition (Theme VARCHAR, Host_city_ID VARCHAR);\\n\\nquestion:Please show the themes of competitions with host cities having populations larger than 5000.\"\n",
    "]\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.01, top_k=50, top_p=0.95)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "# Test inference\n",
    "for prompt in prompts:\n",
    "    print(f\"    Prompt:\\n{prompt}\")\n",
    "    print(f\"    Response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "#-------------Pushing the model to Huggingface Hub-----#\n",
    "login(token=\"\")\n",
    "model.push_to_hub(\"SQLGemma-2-2b-it\", use_temp_dir=False)\n",
    "tokenizer.push_to_hub(\"SQLGemma-2-2b-it\", use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference from HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Testing Inference from HuggingFace hub-----------------#\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Sarathbabu-Karunanithi/SQLGemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Sarathbabu-Karunanithi/SQLGemma-2-2b-it\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompts = [\n",
    "    # \"What is Fine Tuning? Explain why thats the case and if it was different in the past?\",\n",
    "    # \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"context: CREATE TABLE employee (emp_id VARCHAR,emp_name VARCHAR,emp_salary INTEGER,emp_department VARCHAR);\\n\\nquestion:What is the sql query to find the employee name with the lowest salary?\",\n",
    "    # \"context:CREATE TABLE city (City_ID VARCHAR, Population INTEGER); CREATE TABLE farm_competition (Theme VARCHAR, Host_city_ID VARCHAR);\\n\\nquestion:Please show the themes of competitions with host cities having populations larger than 5000.\"\n",
    "]\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.01, top_k=50, top_p=0.95)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "# Test inference\n",
    "for prompt in prompts:\n",
    "    print(f\"    Prompt:\\n{prompt}\")\n",
    "    print(f\"    Response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing and uploading to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Quantizing and uploading to Huggingface Hub--------------#\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"Sarathbabu-Karunanithi/SQLGemma-2-2b-it\"\n",
    "# tokenizer_id = \"philschmid/gemma-tokenizer-chatml\"\n",
    "tokenizer_id = \"Sarathbabu-Karunanithi/SQLGemma-2-2b-it\"\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "model.save_pretrained(\"SQLGemma-2-2b-it\")\n",
    "tokenizer.save_pretrained(\"SQLGemma-2-2b-it\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
